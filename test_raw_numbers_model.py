#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script test mÃ´ hÃ¬nh raw_numbers cáº£i tiáº¿n vá»›i 255 dá»± Ä‘oÃ¡n
"""

import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
import os
import glob

def load_recent_data(data_file="data-dacbiet.txt", num_recent=10):
    """Äá»c dá»¯ liá»‡u gáº§n nháº¥t tá»« file"""
    if not os.path.exists(data_file):
        print(f"KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {data_file}")
        return []
    
    with open(data_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # Lá»c vÃ  chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u
    numbers = []
    for line in lines:
        line = line.strip()
        if line.isdigit() and len(line) == 3:
            numbers.append(int(line))
    
    # Tráº£ vá» sá»‘ gáº§n nháº¥t
    return numbers[-num_recent:]

def test_raw_numbers_model(model_path, scaler_path, recent_data):
    """Test mÃ´ hÃ¬nh raw_numbers vá»›i 255 dá»± Ä‘oÃ¡n"""
    print(f"\nğŸ”¢ TESTING RAW_NUMBERS MODEL:")
    print(f"Model: {os.path.basename(model_path)}")
    
    try:
        # Load model vÃ  scaler
        model = tf.keras.models.load_model(model_path)
        scaler = np.load(scaler_path, allow_pickle=True).item()
        
        # Chuáº©n hÃ³a dá»¯ liá»‡u Ä‘áº§u vÃ o
        numbers_normalized = scaler.transform(np.array(recent_data).reshape(-1, 1)).flatten()
        
        # Dá»± Ä‘oÃ¡n vá»›i randomness
        predictions = []
        current_sequence = numbers_normalized[-10:].reshape(1, 10, 1)
        
        print("ğŸ”„ Äang thá»±c hiá»‡n dá»± Ä‘oÃ¡n 255 sá»‘...")
        
        for i in range(255):
            if i % 50 == 0:  # Hiá»ƒn thá»‹ tiáº¿n Ä‘á»™
                print(f"  ÄÃ£ dá»± Ä‘oÃ¡n {i}/255 sá»‘...")
            
            pred = model.predict(current_sequence, verbose=0)
            
            # Sá»­ dá»¥ng temperature scaling Ä‘á»ƒ tÄƒng randomness
            temperature = 1.5
            pred_scaled = pred[0] / temperature
            pred_probs = np.exp(pred_scaled) / np.sum(np.exp(pred_scaled))
            
            # Láº¥y top 5 predictions vÃ  chá»n ngáº«u nhiÃªn
            top_5_indices = np.argsort(pred_probs)[-5:][::-1]
            top_5_probs = pred_probs[top_5_indices]
            
            # Chá»n ngáº«u nhiÃªn tá»« top 5 vá»›i xÃ¡c suáº¥t tÆ°Æ¡ng á»©ng
            chosen_idx = np.random.choice(top_5_indices, p=top_5_probs/np.sum(top_5_probs))
            pred_normalized = chosen_idx / 999.0
            
            # Chuyá»ƒn vá» sá»‘ nguyÃªn
            pred_original = int(scaler.inverse_transform([[pred_normalized]])[0][0])
            predictions.append(pred_original)
            
            # Cáº­p nháº­t chuá»—i
            current_sequence = np.roll(current_sequence, -1, axis=1)
            current_sequence[0, -1, 0] = pred_normalized
        
        print(f"âœ… Dá»± Ä‘oÃ¡n thÃ nh cÃ´ng: {len(predictions)} sá»‘")
        
        # Hiá»ƒn thá»‹ 10 sá»‘ Ä‘áº§u vÃ  10 sá»‘ cuá»‘i
        print(f"ğŸ“Š 10 sá»‘ Ä‘áº§u tiÃªn: {predictions[:10]}")
        print(f"ğŸ“Š 10 sá»‘ cuá»‘i cÃ¹ng: {predictions[-10:]}")
        
        # Kiá»ƒm tra tÃ­nh Ä‘a dáº¡ng
        unique_predictions = len(set(predictions))
        print(f"ğŸ“Š Sá»‘ dá»± Ä‘oÃ¡n khÃ¡c nhau: {unique_predictions}/255")
        
        if unique_predictions >= 200:
            print("ğŸ‰ Tuyá»‡t vá»i! MÃ´ hÃ¬nh ráº¥t Ä‘a dáº¡ng!")
        elif unique_predictions >= 150:
            print("ğŸ‘ Tá»‘t! MÃ´ hÃ¬nh Ä‘Ã£ Ä‘a dáº¡ng hÆ¡n!")
        elif unique_predictions >= 100:
            print("ğŸ‘ MÃ´ hÃ¬nh Ä‘Ã£ cáº£i thiá»‡n!")
        else:
            print("âš ï¸  MÃ´ hÃ¬nh váº«n cÃ²n láº·p láº¡i nhiá»u")
        
        # LÆ°u káº¿t quáº£ dá»± Ä‘oÃ¡n vÃ o file
        output_file = "predictions_255_numbers.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# Dá»± Ä‘oÃ¡n 255 sá»‘ xá»• sá»‘ tá»« mÃ´ hÃ¬nh raw_numbers\n")
            f.write(f"# Model: {os.path.basename(model_path)}\n")
            f.write(f"# Dá»¯ liá»‡u gáº§n nháº¥t: {recent_data}\n")
            f.write(f"# Sá»‘ dá»± Ä‘oÃ¡n khÃ¡c nhau: {unique_predictions}/255\n")
            f.write(f"# Thá»i gian: {os.popen('date').read().strip()}\n\n")
            
            for i, pred in enumerate(predictions, 1):
                f.write(f"{i:3d}: {pred:03d}\n")
        
        print(f"ğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ vÃ o file: {output_file}")
            
    except Exception as e:
        print(f"âŒ Lá»—i: {str(e)}")

def main():
    """HÃ m chÃ­nh"""
    print("=== TEST MÃ” HÃŒNH RAW_NUMBERS Cáº¢I TIáº¾N ===\n")
    
    # Táº£i dá»¯ liá»‡u gáº§n nháº¥t
    recent_data = load_recent_data()
    if not recent_data:
        print("KhÃ´ng thá»ƒ Ä‘á»c dá»¯ liá»‡u gáº§n nháº¥t")
        return
    
    print(f"ğŸ“Š Dá»¯ liá»‡u gáº§n nháº¥t ({len(recent_data)} sá»‘): {recent_data}")
    
    # TÃ¬m mÃ´ hÃ¬nh raw_numbers má»›i nháº¥t
    raw_models = glob.glob("lottery_model_raw_numbers_*.keras")
    if not raw_models:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y mÃ´ hÃ¬nh raw_numbers!")
        return
    
    # Sáº¯p xáº¿p theo thá»i gian sá»­a Ä‘á»•i
    raw_models.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    latest_model = raw_models[0]
    
    print(f"\nğŸ” TÃ¬m tháº¥y mÃ´ hÃ¬nh:")
    print(f"  raw_numbers: {os.path.basename(latest_model)}")
    
    # Test mÃ´ hÃ¬nh raw_numbers
    scaler_path = latest_model.replace('.keras', '_scaler.npy')
    if os.path.exists(scaler_path):
        test_raw_numbers_model(latest_model, scaler_path, recent_data)
    else:
        print(f"\nâŒ KhÃ´ng tÃ¬m tháº¥y scaler cho raw_numbers")
    
    print(f"\n{'='*60}")
    print("ğŸ¯ Káº¾T LUáº¬N:")
    print("âœ… MÃ´ hÃ¬nh raw_numbers Ä‘Ã£ Ä‘Æ°á»£c cáº£i tiáº¿n vá»›i:")
    print("   - Regularization (L2, BatchNormalization)")
    print("   - Data augmentation (noise, rotation)")
    print("   - Temperature scaling (randomness)")
    print("   - Top-k sampling thay vÃ¬ argmax")
    print("   - Kiáº¿n trÃºc tá»‘i Æ°u hÃ³a")
    print("   - Dá»± Ä‘oÃ¡n 255 sá»‘ thay vÃ¬ 5 sá»‘")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
